\chapter{Related Work}
\label{chap:related}
This chapter gives an overview of the related work of the thesis. We
start with the knowledge processing systems KnowRob and OpenRobots
Ontology, which provide similar functionality. Afterwards we present
the Generic Robot Database based on MongoDB and Fawkes, and Open-EASE,
a combination of KnowRob and the Generic Robot Database.
\todo[inline]{mention added sections}

\section{KnowRob}
\label{sec:knowrob}
KnowRob is an open source knowledge processing system for
cognition-enabled robots~\cite{KnowRob,KnowRob-Representation}. It is
designed to store knowledge about the environment and set it into
relation to common sense knowledge to understand vaguely described
tasks, such as "set the table". For representation and inference of
knowledge, KnowRob uses Description Logic and approaches of the
semantic web.  Each small piece of information is represented as a
\emph{Resource Description Framework (RDF) triple} (e.g. $rdf(robot,
holding, cup)$) that are composed of a subject, predicate, and
object. A set of these RDF triples in a network can compose
commonsense knowledge based on the \emph{Web Ontology Language
  (OWL)}~\cite{owl}. It uses \emph{ontologies}, which can roughly be described as
directed graphs setting objects or classes of objects into
relation. The graph can be represented with multiple RDF triples,
where the subject and object are nodes and the predicate is a directed
edge from the subject to the object. In these ontologies, it is
possible to represent common sense knowledge, such as "milk is-a
perishable", "refrigerator storage-place-for perishable", and
"refrigerator is-a box-container". Thus a robot asked to bring milk
could infer that the milk is stored in the refrigerator, which needs
to be opened first since it is a box-like container.

The implementation and interface of KnowRob is based on the logic
programming language Prolog, which makes representing RDF triples and
inference with ontologies intuitive.  To be able to interface with
perception, KnowRob uses a concept called \emph{virtual knowledge
  base}. It allows computing needed information on demand instead of
storing everything providently. Thus special queries can be forwarded
to other components that compute the answer efficiently.  Especially
for perception based on sensor data or transformation of spatial
relations, this is advantageous compared to computing and storing all
possibly needed results in the memory.  KnowRob implements it with
Prolog predicates called \emph{Computables} which call C++ functions
of other components. This indeed slows down the computation of a query
but is more efficient than continuously computing and storing the
results or implementing and computing other components algorithms in
Prolog.  The concept inspired the usage of computables in this thesis.

To understand the robots environment, large and detailed ontologies
are necessary. KnowRob features acquiring and connecting these from
various sources, such as online common sense databases, the
cloud-based robot infrastructure RoboEarth~\cite{roboearth} and
websites for deriving object information from internet shops and
action recipes from how-to websites giving step by step instructions.
However, it has been found that this acquired knowledge needs a lot of
manual extension and verification before it can be used by the
robot~\cite{KnowRob-Web}. Encyclopedic knowledge often lacks action
information needed by the robot (e.g. how to grab tools) and
action recipes require human text understanding.
\todo[inline]{executive integration}
Furthermore large ontologies limit the performance of KnowRob because
of the Prolog implementation and general complexity. In contrast to
the robot memory of this thesis, KnowRob focuses on common
sense reasoning instead of an efficient and flexible on-line
back-end. It does not support sharing knowledge between multiple
robots and has a too strong focus on data only usable for reasoning
components. Although it can also represent spatio-temporal knowledge,
we have performance and scalability concerns with Prolog handling
larger data-sets (e.g. for storing locations of found object over long
time to learn the distribution where they can be found). Although
KnowRob can load ontologies from a hard drive, the memory acquired
during runtime is not persistently stored.

\section{OpenRobots Ontology (ORO)}
\label{sec:oro}
The OpenRobots Ontology (ORO) is an open source knowledge processing
framework for robotics~\cite{Oro}. It is designed to enable
human-robot interaction by featuring commonsense ontologies similar to
KnowRob and modeling the human point of view. It also uses RDF triples
and OWL ontologies.  The triple storage is implemented in Jena, a Java
toolkit for RDF, and the inference with Pellet, a Java OWL
reasoner. Besides knowledge storing, querying and reasoning, ORO
features a modular architecture between the back-end storage and front
end socket server for querying. These modules add features such as
events that notify external components about changes (e.g. if
instances of certain classes are added or modified).
Another module adds representations of alternative perspectives that
should allow understanding the point of view of other agents.
For example when a human asks a robot to bring a
cup, there are two cups on a table and the robot should infer which
cup is meant by knowing that the other cup is occluded from the human's
point of view. Furthermore ORO features categorization to find
differences between objects (e.g. to ask if the blue cup or the red
cup was meant in a vague task description) and memory profiles for
distinguishing long-term knowledge and short-term knowledge that is
removed when the lifetime of a fact expires. Although these are useful
features and we realized the concepts of events and memory
profiles in this thesis, we do not use ORO as a basis. Due to its
focus on human-robot interaction and commonsense reasoning, it is not
suitable for representing large amounts of data for non-reasoning
components because all data would have to be stored in RDF triples and
processed by the OWL reasoner. Furthermore, it does neither support
synchronizing a part of the knowledge with other robots nor knowledge
computation on demand.

\section{Generic Robot Database with MongoDB}
\label{sec:mongo-logging}
There already is a generic robot database developed with MongoDB as
data storage~\cite{RoboDB}. It is used to log data of the robot
middlewares Fawkes and ROS, and allows fault analysis and performance
evaluation. To achieve this, it taps into the messaging infrastructure
in ROS and the blackboard interfaces in Fawkes to store the data one
to one utilizing the flexibility of the schema free MongoDB. The
logged data can later be queried by the robot or a developer. This
allows better evaluation and fault analysis because the data would
otherwise be disposed and logging of the whole
Data-Information-Knowledge-Wisdom (DIKW) hierarchy~\cite{DIKW} enables
retracing processes, such as from point clouds (data) to cluster
positions (information) to object positions (knowledge) combined from
multiple clues to learning results (wisdom). This work already
implemented a basic connection to MongoDB in Fawkes for storing
documents and executing queries, which can is used and extended in
this thesis. It also shows that MongoDB is a good choice because it
can handle querying a large database and writing a lot of data
efficiently while being flexible regarding how and which kind of data
is being represented.  However this work lacks some important concepts
needed for a robot memory as intended in this thesis. It is intended
as logging facility and not as working memory holding a world model
for multiple planners and reasoners. Therefore updating and querying
mechanisms for planners and reasoners are missing as well as triggers,
a knowledge computable on demand, multi-robot synchronization, a
suitable front-end, ad integration into knowledge based systems.

\section{Open-EASE}
\label{sec:openease}
Open-EASE is a knowledge processing service for robots and AI
researchers~\cite{OpenEASE}. It is based on and combines
KnowRob~\cite{KnowRob} and the Generic Robot Database with MongoDB in
ROS~\cite{RoboDB}. It is designed as a web service accessible by
robots via a socket connection and by humans via a web browser. It
uses KnowRob for commonsense reasoning and storing the world model of
a robot. The Generic Robot Database is used to log data about robots
or humans performing manipulation tasks. This data can then be
accessed by using the virtual knowledge base concept to learn from
human demonstration and analyze faults. The focus of Open-EASE also is
on providing the recorded data and access to KnowRob to humans using
the web interface. This allows using Open-EASE as eLearning tool for
students, who can explore and experiment with the data and system on
the robot. Furthermore it fosters reproducing experiment results
(e.g. for reviews) and visualizing them. Although the system is
accessible from multiple users and robots, which can query the same
Open-EASE instance, it is focused on single robot data and non
collaborative processes because each user operates in his private
container with individual knowledge base. This makes sense in the
context of student exercises, but is not suitable for multi-robot
systems or multiple knowledge based systems sharing knowledge. If the
system is also usable or being extended for cooperating robots or
planners exchanging knowledge over the web service is not mentioned in
current publications or on the project
website\footnote{\url{http://www.open-ease.org/}}.

\section{Choise for Underlying Basis System}
\label{sec:basis-representation}
In this section, we compare data storage systems that can be used as a
basis for the robot memory of this thesis and explain why we chose a
document oriented database and MongoDB in particular. The possible
systems we compare here belong to the context already introduced in
the background and related work and are production systems,
ontologies, relational databases, and document oriented databases.

One possibility for an underlying basis system is a production system
and CLIPS in particular. The information stored in the robot memory
can be represented by facts in the volatile fact base. In these facts,
key-value pairs can be used according to a predefined fact
template. The main advantage of a production system is it's reasoning
capability that allows infering more information and knowledge from
what is inserted and combining it with the existing fact base. For
example, this would allow inserting new observations and deriving an
updated world model in clips which can later be extracted. However,
this would be domain dependent since rules for updating and infering
would have to be developed. Compared to other data storage systems,
CLIPS lacks query features. By iterating over the fact base and
comparing template names and values, fact extraction can be
implemented. However, this would lack query features known from
databases and probably not scale well for large datasets due to the
lack of indexing. The rule matching capabilities of CLIPS are not
suitable for querying because each query would reqire a new rule that
changes the graph underlying the RETE algorithm and thus causes a
large computational overhead~\cite{Rete}.

Ontologies and RDF triples from the Web Ontology Language are another
possibility for a data storage system of the robot memory. Especially
KnowRob uses these and was considered as a basis for this
thesis. Information can be represented as RDF triples and embedded in
an ontology graph. The main advantage is, that this allows reasoning
while answering a query because the query can be reformulated using
backwards chaining with the ontology as a basis until a solution is
found or there are no more possibilities. A disadvantages of this is
the scalability with the stored data size and ontology size. For
example, the robot memory should be able to store a large dataset
about object sights to learn their spatio-temporal
distribution. Furthermore, the information is not stored persistently
and can not be distributed in a multi robot system.

Compared to the previous approaches with ontologies and production
systems, database systems focus on data storage and querying instead
of reasoning. We think that they are a better basis for a robot memory
because they allow separating the concerns of storing and remembering
on the one hand, and reasoning, planning, and other robotic
applications on the other hand. They are designed to work as efficient
data storages with rich querying and distribution features. The
separation makes it possible for multiple systems to work with the
robot memory and to collaborate over it. When for example the common
sense reasoning of KnowRob should be integrated into a robot system,
the robot memory can store the required information in the database
and provide KnowRob with the required information and
ontologies. Furthermore, other systems such as a PDDL planner or a
component learning object distributions from sights can also use the
common basis in an efficient way. Before chosing a concrete database
managing system, we have to decide between a relational database and a
document-oriented database. Graph databases are not suited very well
because the information we want to represent often does not have a
graph structure, such as a world model of the RCLL or memorized object
sights. Thus the main advantages of graph databases, namely searching
and traversing a graph, are not so important.  In contrast to document
oriented databases, relational ones provide powerfull query features,
such as joining over multiple relations. Document oriented databases
are schema free and thus allow a higher flexibility during development
and runtime. This flexibility is important because representations of
information on the robot tend to change during development and it is
posible to group documents with similar purpose but different
attributes together (e.g. facts of a world model). It also avoids
joining over many relations by storing by storing information that
belongs together as single document (e.g. with nesting).  Furthermore,
document oriented databases provide a simpler possibility to
distribute the system over multiple robots. These arguments lead to
the choice of a document oriented database. Why we prefer MongoDB
among other document oriented database systems was discussed in
\refsec{sec:mongodb}. The main reasons are the performance,
scalability and popularity of MongoDB as well as it's query language.

\section{MongoDB Extensions}
\label{sec:mongodb-extensions}
The work of this thesis uses MongoDB as a basis and extends it with
some concepts that are important for the usage as robot memory. In
this section, we present MongoDB extensions which are related to our
extensions, such as triggers for notifications about changes
in the database.

\subsection{Trigger}
\label{sec:mongodb-trigger}
In the database jargon, a trigger is a piece of code that is
automatically executed after a specified event, such as a change in
the database. Thus a trigger allows the user to react to events
without polling. Triggers are common in relational database management
systems, such as PostgreSQL~\cite{postgresql}. MongoDB does not
provide triggers, but there are multiple projects prividing them as
extension (e.g.~\cite{mongodb-trigger}). A simple way to realize
triggers on insertion events is using capped collections of MongoDB
because they order documents by the time of
insertion. \todo[inline]{insert, link figure of capped collection?} The lazy and
iterative query evaluation strategy of MongoDB allows querying the
capped collection and keeping the cursor if there are no more
documents. After some time, the query can be continued from the
cursors position to check all documents that were inserted in the
meantime because they were just appended after the documents inserted
before. Conviniently MongoDB uses a capped collection to synchronize
changes in a replica set. This capped collection, called the
\emph{Oplog}, can also be queried by the user and allows listening to
changes in the replica set as described above.

\subsection{Multi-Master Replication}
\label{sec:mongodb-multi-master}
There also exist an extensions of MongoDB which implement a
\emph{Multi-Master Replication}. It is called \emph{MongoDB
  MultiMaster (MMM)}\footnote{\url{https://github.com/rick446/mmm}}
and enables writing to each instance of a distributed MongoDB
database~\cite{mongodb-multi-master}. Changes can be immediately
applied locally and are synchronized later to the other
instances. This allows faster writing especially if the other
instances are not available for some time and would be advantagous in
the RCLL, where a clouded WiFi can lead to larger latencies. However,
MMM is not focused on consistency and ignores write conflicts. Other
disadvantages are increased overall communication and database
size. This is caused by the implementation of MMM, which uses an usual
Master-Slave replication for each instance. Each instance consists of
one Master of these replications and Slaves for the other. MMM then
applies changes in a Slave replication to the local Master.  To detect
changes in a replica, MMM utilizes MongoDBs Oplog similar to this
thesis. It queries the Oplog, which is capped collection, as described
in the \refsec{sec:mongodb-trigger}.

\todo[inline]{wirklich kein related work zu on demand computation in
  mongodb?}
